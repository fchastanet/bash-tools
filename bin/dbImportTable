#!/usr/bin/env bash

CURRENT_DIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
# load bash-framework
# shellcheck source=bash-framework/_bootstrap.sh
source "$( cd "${CURRENT_DIR}/.." && pwd )/bash-framework/_bootstrap.sh"

Framework::expectNonRootUser


import bash-framework/Database

# ensure that Ctrl-C is trapped by this script and not sub mysql process
trap 'exit 130' INT

# default values
SCRIPT_NAME=${0##*/}
FORCE=0
FROM_AWS=0
DOWNLOAD_DUMP=0
REMOTE_DB=""
LOCAL_DB=""
COLLATION_NAME=""
CHARACTER_SET=""
TABLE=""
FROM_DSN=""
DEFAULT_FROM_DSN="default.remote"
TARGET_DSN="default.local"
TIMEFORMAT='time spent : %3R'
# remove last slash
DB_IMPORT_DUMP_DIR=${DB_IMPORT_DUMP_DIR%/}

# check dependencies
Functions::checkCommandExists mysql "sudo apt-get install mysql-client"
Functions::checkCommandExists mysqlshow "sudo apt-get install mysql-client"
Functions::checkCommandExists mysqldump "sudo apt-get install mysql-client"# check dependencies
Functions::checkCommandExists pv "sudo apt-get install -y pv"

showHelp() {
cat << EOF
Description: Import remote db table into local db

Command: ${SCRIPT_NAME} [-h|--help] prints this help and exits
Command: ${SCRIPT_NAME} <remoteDbName> <tableName> [<localDbName>] 
    [-d|--download-dump] [--force] [-a|--from-aws]
    [-t|--target-dsn dsn] [-f|--from-dsn dsn]
    [-o|--collation-name utf8_general_ci] [-c|--character-set utf8]

    download the remote table data and install data in local database (the schema should exists)

    <tableName>   : table name to import
    <localDbName> : use remote db name if not provided
    --force If local table exists, it will overwrite it
    -t|--target-dsn dsn         dsn to use for target database (Default: ${TARGET_DSN}) 
    -f|--from-dsn dsn           dsn to use for source database (Default: ${DEFAULT_FROM_DSN})
        this option is incompatible with -a|--from-aws option
    -a|--from-aws db dump will be downloaded from s3 instead of using remote db, 
        remoteDBName will represent the name of the file
        profile will be calculated against the dump itself
    -d|--download-dump force remote db dump (default: use already downloaded dump in ${DB_IMPORT_DUMP_DIR} if available)
    -o|--collation-name change the collation name used during database creation (default value: collation name used by remote db)
    -c|--character-set change the character set used during database creation (default value: character set used by remote db)

    Aws s3 location       : ${S3_BASE_URL}
EOF
}

# read command parameters
# $@ is all command line parameters passed to the script.
# -o is for short options like -h
# -l is for long options with double dash like --help
# the comma separates different long options
options=$(getopt -l help,force,target-dsn:,from-dsn:,from-aws,download-dump,collation-name:,character-set: -o aht:f:dc:o: -- "$@" 2> /dev/null) || {
    showHelp
    Log::fatal "invalid options specified"
}

eval set -- "${options}"
while true
do
case $1 in
-h|--help)
    showHelp
    exit 0
    ;;
--force)
    Log::displayInfo "If local db exists, it will overwrite it"
    FORCE=1
    ;;
-t|--target-dsn)
    shift
    TARGET_DSN="$1"
    ;;
-f|--from-dsn)
    shift
    FROM_DSN="${1:-${DEFAULT_FROM_DSN}}"
    ;;
-a|--from-aws)
    FROM_AWS="1"
    ;;
-d|--download-dump)
    DOWNLOAD_DUMP=1
    ;;
-o|--collation-name)
    shift
    COLLATION_NAME="$1"
    ;;
-c|--character-set)
    shift
    CHARACTER_SET="$1"
    ;;
--)
    shift
    break;;
*)
    showHelp
    Log::fatal "invalid argument $1"
esac
shift
done

# additional arguments
shift $(expr $OPTIND - 1 )
while true; do
    if [[ -z "$1" ]]; then
        # last argument
        break
    fi
    if [[ -z "${REMOTE_DB}" ]]; then
        REMOTE_DB="$1"
    else
        if [[ -z "${TABLE}" ]]; then
            TABLE="$1"
        else
            LOCAL_DB="$1"
        fi
    fi

    shift
done

if [[ -z "${REMOTE_DB}" ]]; then
    showHelp
    Log::fatal "you must provide remoteDbName"
fi

if [[ -z "${TABLE}" ]]; then
    showHelp
    Log::fatal "you must provide tableName"
fi

if [[ -z "${LOCAL_DB}" ]]; then
    LOCAL_DB=${REMOTE_DB}
fi

if [[ "${MYSQL_HOSTNAME}" = "localhost" ]]; then
    Log::displayWarning "check that MYSQL_HOSTNAME should not be 127.0.0.1 instead of localhost"
fi

# check s3 parameter
if [[ "${FROM_AWS}" = "1" && -n "${FROM_DSN}" ]]; then
    Log::fatal "you cannot use from-dsn and from-aws at the same time"
fi
if [[ "${FROM_AWS}" = "1" && -z "${S3_BASE_URL}" ]]; then
    Log::fatal "missing S3_BASE_URL, please provide a value in .env file"
fi

# default value for FROM_DSN if from-aws not set
if [[ "${FROM_AWS}" = "0" && -z "${FROM_DSN}" ]]; then
    FROM_DSN="${DEFAULT_FROM_DSN}"
fi

if [[ -z "${DB_IMPORT_DUMP_DIR}" ]]; then
    Log::fatal "you have to specify a value for DB_IMPORT_DUMP_DIR env variable"
fi
if [[ ! -d "${DB_IMPORT_DUMP_DIR}" ]]; then
    mkdir -p "${DB_IMPORT_DUMP_DIR}" || 
        Log::fatal "impossible to create directory ${DB_IMPORT_DUMP_DIR} specified by DB_IMPORT_DUMP_DIR env variable"
fi

# create db instances
declare -Agx dbRemoteInstance dbLocalInstance
Database::newInstance dbTargetDatabase "${TARGET_DSN}"
Database::setOptions dbTargetDatabase "${MYSQL_OPTIONS} --connect-timeout=5"
if [[ "${FROM_AWS}" = "0" ]]; then
    Database::newInstance dbFromInstance "${FROM_DSN}"
    Database::setOptions dbFromInstance "${MYSQL_OPTIONS} --connect-timeout=5"
fi

# check if local db exists
Database::ifDbExists dbTargetDatabase "${LOCAL_DB}" || {
    Log::displayWarning "Local Database ${LOCAL_DB} does not exist !"
}

# check if local table exists
Database::isTableExists dbLocalInstance "${LOCAL_DB}" "${TABLE}" && {
    Log::displayInfo "Local table ${LOCAL_DB}/${TABLE} already exists"
    [[ "${FORCE}" = "0" ]] && Log::fatal "use --force to drop it"
}

Log::displayInfo "import table ${TABLE} from ${REMOTE_DB} to ${LOCAL_DB}"

# check if local dump exists
REMOTE_DB_DUMP_TEMP_FILE="${DB_IMPORT_DUMP_DIR}/importTable_${REMOTE_DB}_${TABLE}.sql"
if [[ "${DOWNLOAD_DUMP}" = "0" ]]; then
    if [[ -f "${REMOTE_DB_DUMP_TEMP_FILE}" ]]; then
        Log::displayInfo "local dump ${REMOTE_DB_DUMP_TEMP_FILE} already exists, avoid download"
    else
        Log::displayInfo "local dump does not exist"
        DOWNLOAD_DUMP=1
    fi
fi

# dump header/footer
read -r -d '\0' DUMP_HEADER <<- EOM
    SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS = 0;
    SET @OLD_AUTOCOMMIT=@@AUTOCOMMIT, AUTOCOMMIT = 0;
    SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS = 0;\0
EOM

read -r -d '\0' DUMP_FOOTER <<- EOM2
    SET AUTOCOMMIT=@OLD_AUTOCOMMIT;
    SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;
    SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;\0
EOM2

SECONDS=0
if [[ "${DOWNLOAD_DUMP}" = "1" ]]; then
    Log::displayInfo "Download dump"

    if [[ "${FROM_AWS}" = "1" ]]; then
        # download dump from s3
        AWS_DUMP_FILE="${DB_IMPORT_DUMP_DIR}/${REMOTE_DB}.tar.gz"
        if [[ ! -f "${AWS_DUMP_FILE}" ]]; then
            S3_URL="${S3_BASE_URL%/}/${REMOTE_DB}.tar.gz"
            Log::displayInfo "Download dump from ${S3_URL} ...";
            TMPDIR="${TEMP_FOLDER:-/tmp}" aws s3 cp "${S3_URL}" "${AWS_DUMP_FILE}" ||
                Log::fatal "unable to download dump from S3 : ${S3_URL}"
        else
            Log::displayInfo "Local full dump exists : ${AWS_DUMP_FILE}"
        fi

        (
            echo "${DUMP_HEADER}"
            if [[ -n "${CHARACTER_SET}" ]]; then
                echo "SET names '${CHARACTER_SET}';"
            fi
            # extract Table from the dump
            "${CURRENT_DIR}/dbImportTableStream" \
                "${AWS_DUMP_FILE}" \
                "${TABLE}" \
                "${CHARACTER_SET}"
            
            echo "${DUMP_FOOTER}"
        ) > "${REMOTE_DB_DUMP_TEMP_FILE}"
        Log::displayInfo "Table dump extracted : ${REMOTE_DB_DUMP_TEMP_FILE}"
    else
        # get remote db collation name
        if [[ -z "${COLLATION_NAME}" ]]; then
            COLLATION_NAME=$(Database::query dbRemoteInstance \
                "SELECT default_collation_name FROM information_schema.SCHEMATA WHERE schema_name = \"${REMOTE_DB}\";" "information_schema")
        fi

        # get remote db character set
        if [[ -z "${CHARACTER_SET}" ]]; then
            CHARACTER_SET=$(Database::query dbRemoteInstance \
                "SELECT default_character_set_name FROM information_schema.SCHEMATA WHERE schema_name = \"${REMOTE_DB}\";" "information_schema")
        fi

        DUMP_HEADER=$(printf "${DUMP_HEADER}\nSET names '${CHARACTER_SET}';\n")

        # check if remote db exists
        Database::ifDbExists dbRemoteInstance "${REMOTE_DB}" || 
            Log::fatal "Remote Database ${REMOTE_DB} does not exist"

        # calculate remote table dump size
        Log::displayDebug "Calculate dump size for table ${TABLE}"
        DUMP_SIZE_QUERY="SELECT ROUND(SUM(data_length + index_length) / 1024 / 1024, 0) AS size FROM information_schema.TABLES WHERE table_schema=\"${REMOTE_DB}\""
        DUMP_SIZE_QUERY+=" AND table_name='${TABLE}'"
        DUMP_SIZE_QUERY+=" GROUP BY table_schema"
        REMOTE_DB_DUMP_SIZE=$(echo "${DUMP_SIZE_QUERY}" | Database::query dbRemoteInstance )
        if [[ -z "${REMOTE_DB_DUMP_SIZE}" ]]; then
            # could occur with the none profile
            REMOTE_DB_DUMP_SIZE="0"
        fi

        # dump db
        Log::displayInfo "Dump the table ${TABLE} from database $REMOTE_DB (Size:${REMOTE_DB_DUMP_SIZE}MB) ... to file ${REMOTE_DB_DUMP_TEMP_FILE}";
        echo "${DUMP_HEADER}" > "${REMOTE_DB_DUMP_TEMP_FILE}"
        DUMP_SIZE_PV_ESTIMATION=$(awk "BEGIN {printf \"%.0f\",${REMOTE_DB_DUMP_SIZE}/1.5}")
        Database::dump dbRemoteInstance "${REMOTE_DB}" "${TABLE}" --skip-add-drop-table --single-transaction=TRUE | \
            pv --progress --size "${DUMP_SIZE_PV_ESTIMATION}m" >> "${REMOTE_DB_DUMP_TEMP_FILE}"
        echo "${DUMP_FOOTER}" >> "${REMOTE_DB_DUMP_TEMP_FILE}"
        sed -i -r -e 's/CREATE TABLE `/CREATE TABLE IF NOT EXISTS `/g' "${REMOTE_DB_DUMP_TEMP_FILE}"
    fi
    Log::displayDebug "Dump done.";
else
    if [[ -z "${COLLATION_NAME}" ]]; then
        COLLATION_NAME="utf8_general_ci"
    fi
fi

Log::displayDebug "import remote to local from file ${REMOTE_DB_DUMP_TEMP_FILE}"
pv "${REMOTE_DB_DUMP_TEMP_FILE}" | \
    Database::query dbLocalInstance "" "${LOCAL_DB}"

Log::displayInfo "Import table duration : $(date -u -d @${SECONDS} +"%T")"
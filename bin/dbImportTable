#!/usr/bin/env bash

CURRENT_DIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
# load bash-framework
# shellcheck source=bash-framework/_bootstrap.sh
source "$( cd "${CURRENT_DIR}/.." && pwd )/bash-framework/_bootstrap.sh"

Framework::expectNonRootUser


import bash-framework/Database

# ensure that Ctrl-C is trapped by this script and not sub mysql process
trap 'exit 130' INT

# default values
SCRIPT_NAME=${0##*/}
FORCE=0
FROM_AWS=0
DOWNLOAD_DUMP=0
REMOTE_DB=""
TARGET_DB=""
CHARACTER_SET=""
TABLE=""
FROM_DSN=""
DEFAULT_FROM_DSN="default.remote"
TARGET_DSN="default.local"
TIMEFORMAT='time spent : %3R'
# remove last slash
DB_IMPORT_DUMP_DIR=${DB_IMPORT_DUMP_DIR%/}

showHelp() {
local dsnList=""
dsnList="$(Functions::getConfMergedList "dsn" "env")"


cat << EOF
${__HELP_TITLE}Description:${__HELP_NORMAL} Import remote db table into local db

${__HELP_TITLE}Usage:${__HELP_NORMAL} ${SCRIPT_NAME} [-h|--help] prints this help and exits
${__HELP_TITLE}Usage:${__HELP_NORMAL} ${SCRIPT_NAME} <remoteDbName> <tableName> [<localDbName>] 
    [-d|--download-dump] [--force] [-a|--from-aws]
    [-t|--target-dsn dsn] [-f|--from-dsn dsn]
    [-c|--character-set utf8]

    download the remote table data and install data in local database (the schema should exists)

    <tableName>   :      table name to import
    <localDbName> :      use remote db name if not provided
    --force              If local table exists, it will overwrite it
    -t|--target-dsn dsn  dsn to use for target database (Default: ${TARGET_DSN}) 
    -f|--from-dsn dsn    dsn to use for source database (Default: ${DEFAULT_FROM_DSN})
        this option is incompatible with -a|--from-aws option
    -a|--from-aws        db dump will be downloaded from s3 instead of using remote db, 
        remoteDBName will represent the name of the file
        profile will be calculated against the dump itself
    -d|--download-dump   force remote db dump (default: use already downloaded dump in ${DB_IMPORT_DUMP_DIR} if available)
    -c|--character-set   change the character set used during database creation (default value: character set used by remote db)

    Aws s3 location       : ${S3_BASE_URL}

${__HELP_TITLE}List of available dsn:${__HELP_NORMAL}
${dsnList}
EOF
}

# read command parameters
# $@ is all command line parameters passed to the script.
# -o is for short options like -h
# -l is for long options with double dash like --help
# the comma separates different long options
options=$(getopt -l help,force,target-dsn:,from-dsn:,from-aws,download-dump,character-set: -o aht:f:dc: -- "$@" 2> /dev/null) || {
    showHelp
    Log::fatal "invalid options specified"
}

eval set -- "${options}"
while true
do
case $1 in
-h|--help)
    showHelp
    exit 0
    ;;
--force)
    Log::displayInfo "If local db exists, it will overwrite it"
    FORCE=1
    ;;
-t|--target-dsn)
    shift || true
    TARGET_DSN="$1"
    ;;
-f|--from-dsn)
    shift || true
    FROM_DSN="${1:-${DEFAULT_FROM_DSN}}"
    ;;
-a|--from-aws)
    FROM_AWS="1"
    ;;
-d|--download-dump)
    DOWNLOAD_DUMP=1
    ;;
-c|--character-set)
    shift || true
    CHARACTER_SET="$1"
    ;;
--)
    shift || true
    break;;
*)
    showHelp
    Log::fatal "invalid argument $1"
esac
shift || true
done

# check dependencies
Functions::checkCommandExists mysql "sudo apt-get install -y mysql-client"
Functions::checkCommandExists mysqlshow "sudo apt-get install -y mysql-client"
Functions::checkCommandExists mysqldump "sudo apt-get install -y mysql-client"
Functions::checkCommandExists pv "sudo apt-get install -y pv"

# additional arguments
shift $(( OPTIND - 1 )) || true
while true; do
    if [[ -z "$1" ]]; then
        # last argument
        break
    fi
    if [[ -z "${REMOTE_DB}" ]]; then
        REMOTE_DB="$1"
    else
        if [[ -z "${TABLE}" ]]; then
            TABLE="$1"
        else
            TARGET_DB="$1"
        fi
    fi
    shift || true
done

if [[ -z "${REMOTE_DB}" ]]; then
    showHelp
    Log::fatal "you must provide remoteDbName"
fi

if [[ -z "${TABLE}" ]]; then
    showHelp
    Log::fatal "you must provide tableName"
fi

if [[ -z "${TARGET_DB}" ]]; then
    TARGET_DB=${REMOTE_DB}
fi

# check s3 parameter
if [[ "${FROM_AWS}" = "1" && -n "${FROM_DSN}" ]]; then
    Log::fatal "you cannot use from-dsn and from-aws at the same time"
fi
if [[ "${FROM_AWS}" = "1" && -z "${S3_BASE_URL}" ]]; then
    Log::fatal "missing S3_BASE_URL, please provide a value in .env file"
fi

# default value for FROM_DSN if from-aws not set
if [[ "${FROM_AWS}" = "0" && -z "${FROM_DSN}" ]]; then
    FROM_DSN="${DEFAULT_FROM_DSN}"
fi

if [[ -z "${DB_IMPORT_DUMP_DIR}" ]]; then
    Log::fatal "you have to specify a value for DB_IMPORT_DUMP_DIR env variable"
fi
if [[ ! -d "${DB_IMPORT_DUMP_DIR}" ]]; then
    mkdir -p "${DB_IMPORT_DUMP_DIR}" || 
        Log::fatal "impossible to create directory ${DB_IMPORT_DUMP_DIR} specified by DB_IMPORT_DUMP_DIR env variable"
fi

# create db instances
declare -Agx dbTargetInstance dbFromInstance

Database::newInstance dbTargetInstance "${TARGET_DSN}"
# shellcheck disable=SC2154
Database::setQueryOptions dbTargetInstance "${dbTargetInstance[QUERY_OPTIONS]} --connect-timeout=5"
Log::displayInfo "Using target dsn ${dbTargetInstance['DSN_FILE']}"
if [[ "${FROM_AWS}" = "0" ]]; then
    Database::newInstance dbFromInstance "${FROM_DSN}"
    # shellcheck disable=SC2154
    Database::setQueryOptions dbFromInstance "${dbFromInstance[QUERY_OPTIONS]} --connect-timeout=5"
    Log::displayInfo "Using from dsn ${dbFromInstance['DSN_FILE']}"
fi

# check if target db exists
Database::ifDbExists dbTargetInstance "${TARGET_DB}" || {
    Log::fatal "Target Database ${TARGET_DB} does not exist !"
}

# check if target table exists
Database::isTableExists dbFromInstance "${TARGET_DB}" "${TABLE}" && {
    Log::displayInfo "Target table ${TARGET_DB}/${TABLE} already exists"
    [[ "${FORCE}" = "0" ]] && Log::fatal "use --force to drop it"
    Database::dropTable dbTargetInstance "${TARGET_DB}" "${TABLE}"
}

Log::displayInfo "import table ${TABLE} from ${REMOTE_DB} to ${TARGET_DB}"

# check if local dump exists
REMOTE_DB_DUMP_TEMP_FILE="${DB_IMPORT_DUMP_DIR}/importTable_${REMOTE_DB}_${TABLE}.sql"
if [[ "${DOWNLOAD_DUMP}" = "0" ]]; then
    if [[ -f "${REMOTE_DB_DUMP_TEMP_FILE}" ]]; then
        Log::displayInfo "local dump ${REMOTE_DB_DUMP_TEMP_FILE} already exists, avoid download"
    else
        Log::displayInfo "local dump does not exist"
        DOWNLOAD_DUMP=1
    fi
fi

# dump header/footer
read -r -d '\0' DUMP_HEADER <<- EOM
SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS = 0;
SET @OLD_AUTOCOMMIT=@@AUTOCOMMIT, AUTOCOMMIT = 0;
SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS = 0;\0
EOM

read -r -d '\0' DUMP_FOOTER <<- EOM2
SET AUTOCOMMIT=@OLD_AUTOCOMMIT;
SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;
SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;\0
EOM2

SECONDS=0
if [[ "${DOWNLOAD_DUMP}" = "1" ]]; then
    Log::displayInfo "Download dump"

    if [[ "${FROM_AWS}" = "1" ]]; then
        # download dump from s3
        AWS_DUMP_FILE="${DB_IMPORT_DUMP_DIR}/${REMOTE_DB}.tar.gz"
        if [[ ! -f "${AWS_DUMP_FILE}" ]]; then
            S3_URL="${S3_BASE_URL%/}/${REMOTE_DB}.tar.gz"
            Log::displayInfo "Download dump from ${S3_URL} ...";
            TMPDIR="${TEMP_FOLDER:-/tmp}" aws s3 cp "${S3_URL}" "${AWS_DUMP_FILE}" ||
                Log::fatal "unable to download dump from S3 : ${S3_URL}"
        else
            Log::displayInfo "Local full dump exists : ${AWS_DUMP_FILE}"
        fi

        (
            echo "${DUMP_HEADER}"
            if [[ -n "${CHARACTER_SET}" ]]; then
                echo "SET names '${CHARACTER_SET}';"
            fi
            # extract Table from the dump
            "${CURRENT_DIR}/dbImportTableStream" \
                "${AWS_DUMP_FILE}" \
                "${TABLE}" \
                "${CHARACTER_SET}"
            
            echo "${DUMP_FOOTER}"
        ) > "${REMOTE_DB_DUMP_TEMP_FILE}"
        Log::displayInfo "Table dump extracted : ${REMOTE_DB_DUMP_TEMP_FILE}"
    else
        # get remote db character set
        if [[ -z "${CHARACTER_SET}" ]]; then
            CHARACTER_SET=$(Database::query dbTargetInstance \
                "SELECT default_character_set_name FROM information_schema.SCHEMATA WHERE schema_name = \"${REMOTE_DB}\";" "information_schema")
        fi

        DUMP_HEADER=$(printf "%s\nSET names '%s';\n" "${DUMP_HEADER}" "${CHARACTER_SET}")

        # check if remote db exists
        Database::ifDbExists dbTargetInstance "${REMOTE_DB}" || 
            Log::fatal "Target Database ${REMOTE_DB} does not exist"

        # calculate remote table dump size
        Log::displayDebug "Calculate dump size for table ${TABLE}"
        DUMP_SIZE_QUERY="SELECT ROUND(SUM(data_length + index_length) / 1024 / 1024, 0) AS size FROM information_schema.TABLES WHERE table_schema=\"${REMOTE_DB}\""
        DUMP_SIZE_QUERY+=" AND table_name='${TABLE}'"
        DUMP_SIZE_QUERY+=" GROUP BY table_schema"
        REMOTE_DB_DUMP_SIZE=$(echo "${DUMP_SIZE_QUERY}" | Database::query dbTargetInstance )
        if [[ -z "${REMOTE_DB_DUMP_SIZE}" ]]; then
            # could occur with the none profile
            REMOTE_DB_DUMP_SIZE="0"
        fi

        # dump db
        Log::displayInfo "Dump the table ${TABLE} from database $REMOTE_DB (Size:${REMOTE_DB_DUMP_SIZE}MB) ... to file ${REMOTE_DB_DUMP_TEMP_FILE}";
        echo "${DUMP_HEADER}" > "${REMOTE_DB_DUMP_TEMP_FILE}"
        DUMP_SIZE_PV_ESTIMATION=$(awk "BEGIN {printf \"%.0f\",${REMOTE_DB_DUMP_SIZE}/1.5}")
        Database::dump dbTargetInstance "${REMOTE_DB}" "${TABLE}" --skip-add-drop-table --single-transaction=TRUE | \
            pv --progress --size "${DUMP_SIZE_PV_ESTIMATION}m" >> "${REMOTE_DB_DUMP_TEMP_FILE}"
        echo "${DUMP_FOOTER}" >> "${REMOTE_DB_DUMP_TEMP_FILE}"
        # shellcheck disable=SC2016
        sed -i -r \
            -e 's/CREATE TABLE `/CREATE TABLE IF NOT EXISTS `/g' \
            "${REMOTE_DB_DUMP_TEMP_FILE}"
    fi
    Log::displayDebug "Dump done.";
fi

Log::displayDebug "import from file ${REMOTE_DB_DUMP_TEMP_FILE}"
pv "${REMOTE_DB_DUMP_TEMP_FILE}" | \
    Database::query dbFromInstance "" "${TARGET_DB}"

Log::displayInfo "Import table duration : $(date -u -d @${SECONDS} +"%T")"
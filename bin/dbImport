#!/bin/bash

# load bash-framework
# shellcheck source=bash-framework/_bootstrap.sh
source "$( cd "$( readlink -e "${BASH_SOURCE[0]%/*}/.." )" && pwd )/bash-framework/_bootstrap.sh"

if [[  "$USER" = "root" ]]; then
    Log::displayError "The script must not be run as root"
    exit 1
fi

CURRENT_DIR=$( cd "$( readlink -e "${BASH_SOURCE[0]%/*}" )" && pwd )
PROFILES_DIR="$(cd "${CURRENT_DIR}/.." && pwd)/dbImportProfiles"
HOME_PROFILES_DIR="${HOME}/.dbImportProfiles"

import bash-framework/Database

# ensure that Ctrl-C is trapped by this script and not sub mysql process
trap 'exit 130' INT

# default values
SCRIPT_NAME=${0##*/}
PROFILE="default"
FORCE=0
DOWNLOAD_DUMP=0
FROM_AWS=0
SKIP_SCHEMA=0
REMOTE_DB=""
REFERENCE_DB=""
LOCAL_DB=""
COLLATION_NAME=""
CHARACTER_SET=""
TIMEFORMAT='time spent : %3R'

# check dependencies
if ! which pv > /dev/null || ! which mysql > /dev/null || ! which mysqlshow > /dev/null || ! which mysqldump > /dev/null ; then
    Log::displayError "one of pv/mysql/mysqldump/mysqlshow programs is not installed, run 'sudo apt install -y pv mysql-client' or use docker container"
    exit 1
fi

showHelp() {
    local profilesList=""
    local homeProfilesList=""

    profilesList=$(cd "${PROFILES_DIR}" && find . -type f -name \*.sh -printf '%P\n' | sed 's/\.sh$//g' | paste -sd "," - | sed -e 's/,/, /g')

    if [[ -d "${HOME_PROFILES_DIR}" ]]; then
        homeProfilesList=$(cd "${HOME_PROFILES_DIR}" && find . -type f -name \*.sh -printf '%P\n' | sed 's/\.sh$//g' | paste -sd "," - | sed -e 's/,/, /g')
    fi
cat << EOF
Command: ${SCRIPT_NAME} --help prints this help and exits
Command: ${SCRIPT_NAME} <remoteDbName> [<localDbName>] [-f|--force] 
                        [-d|--download-dump] [-a|--from-aws]
                        [-s|--skip-schema] [-p|--profile profileName] 
                        [-o|--collation-name utf8_general_ci] [-c|--character-set utf8]

    <localDbName> : use remote db name if not provided
    -f|--force If local db exists, it will overwrite it
    -d|--download-dump force remote db dump (default: use already downloaded dump in ${DB_IMPORT_DUMP_DIR} if available)
    -a|--from-aws db dump will be downloaded from s3 instead of using remote db, 
        remoteDBName will represent the name of the file
        profile will be calulated against local db using AWS_REFERENCE_DB=${AWS_REFERENCE_DB} db
    -s|--skip-schema avoid to import the schema
    -o|--collation-name change the collation name used during database creation (default value: collation name used by remote db)
    -c|--character-set change the character set used during database creation (default value: character set used by remote db)
    -p|--profile profileName the name of the profile to use in order to include or exclude tables
        (if not specified ${HOME_PROFILES_DIR}/default.sh  is used if exists otherwise ${PROFILES_DIR}/default.sh)
        list of available home profiles (${HOME_PROFILES_DIR}): ${homeProfilesList}
        list of available profiles : ${profilesList}

    local DB connection   : ${MYSQL_ROOT_USER}:${MYSQL_ROOT_PASSWORD}@${MYSQL_HOSTNAME}:${MYSQL_PORT}
    remote DB connection  : ${REMOTE_MYSQL_USER}:${REMOTE_MYSQL_PASSWORD}@${REMOTE_MYSQL_HOSTNAME}:${REMOTE_MYSQL_PORT}
    Aws s3 location       : ${S3_BASE_URL}
    Aws reference DB name : ${AWS_REFERENCE_DB} (using local DB connection)
EOF
}

# read command parameters
# $@ is all command line parameters passed to the script.
# -o is for short options like -h
# -l is for long options with double dash like --help
# the comma separates different long options
options=$(getopt -l help,force,from-aws,download-dump,skip-schema,profile:,collation-name:,character-set: -o ahfdsp:c:o: -- "$@" 2> /dev/null) || {
    Log::displayError "invalid options specified"
    showHelp
    exit 1
}

eval set -- "${options}"
while true
do
case $1 in
-h|--help)
    showHelp
    exit 0
    ;;
-f|--force)
    Log::displayInfo "If local db exists, it will overwrite it"
    FORCE=1
    ;;
-d|--download-dump)
    DOWNLOAD_DUMP="1"
    ;;
-a|--from-aws)
    FROM_AWS="1"
    # structure is included in s3 file
    SKIP_SCHEMA="1"
    ;;
-s|--skip-schema)
    SKIP_SCHEMA="1"
    ;;
-p|--profile)
    shift
    PROFILE="$1"
    ;;
-o|--collation-name)
    shift
    COLLATION_NAME="$1"
    ;;
-c|--character-set)
    shift
    CHARACTER_SET="$1"
    ;;
--)
    shift
    break;;
*)
    Log::displayError "invalid argument $1"
    showHelp
    exit 1
esac
shift
done

# additional arguments
shift $(expr $OPTIND - 1 )
while true; do
    if [[ -z "$1" ]]; then
        # last argument
        break
    fi
    if [[ -z "${REMOTE_DB}" ]]; then
        REMOTE_DB="$1"
    else
        LOCAL_DB="$1"
    fi

    shift
done

if [[ -z "${REMOTE_DB}" ]]; then
    Log::displayError "you must provide remoteDbName"
    showHelp
    exit 1
fi

if [[ -z "${LOCAL_DB}" ]]; then
    LOCAL_DB=${REMOTE_DB}
fi

#check that remote db env variables are set
if [[ -z "${REMOTE_MYSQL_HOSTNAME}" || -z "${REMOTE_MYSQL_PORT}" || -z "${REMOTE_MYSQL_USER}" || -z "${REMOTE_MYSQL_PASSWORD}" ]]; then
    Log::displayError "missing remote mysql parameters, please provide a value in .env file"
    exit 1
fi

if [[ "${MYSQL_HOSTNAME}" = "localhost" ]]; then
    Log::displayWarning "check that MYSQL_HOSTNAME should not be 127.0.0.1 instead of localhost"
fi

# check s3 parameter
if [[ "${FROM_AWS}" = "1" && -z "${S3_BASE_URL}" ]]; then
    Log::displayError "missing S3_BASE_URL, please provide a value in .env file"
    exit 1
fi

if [[ "${FROM_AWS}" = "1" ]]; then
    REFERENCE_DB="${AWS_REFERENCE_DB}"
else
    REFERENCE_DB="${REMOTE_DB}"
fi

# load the profile
if [[ -z "${PROFILE}" ]]; then
    Log::displayError "you should specify a profile"
    showHelp
    exit 1
fi

if [[ -f "${HOME_PROFILES_DIR}/${PROFILE}.sh" ]]; then
    PROFILE_COMMAND="${HOME_PROFILES_DIR}/${PROFILE}.sh"
elif [[ -f "${PROFILES_DIR}/${PROFILE}.sh" ]]; then
    PROFILE_COMMAND="${PROFILES_DIR}/${PROFILE}.sh"
else
    Log::displayError "the profile ${PROFILES_DIR}/${PROFILE}.sh does not exist"
    exit 1
fi

# create db instances
declare -Agx dbRemoteInstance dbLocalInstance
Database::newInstance dbLocalInstance "${MYSQL_HOSTNAME}" "${MYSQL_PORT}" "${MYSQL_ROOT_USER}" "${MYSQL_ROOT_PASSWORD}"
Database::setOptions dbLocalInstance "${MYSQL_OPTIONS} --connect-timeout=5"
if [[ "${FROM_AWS}" = "1" ]]; then
    Database::newInstance dbRemoteInstance "${MYSQL_HOSTNAME}" "${MYSQL_PORT}" "${MYSQL_ROOT_USER}" "${MYSQL_ROOT_PASSWORD}"
    Database::setOptions dbRemoteInstance "${MYSQL_OPTIONS} --connect-timeout=5"
else
    Database::newInstance dbRemoteInstance "${REMOTE_MYSQL_HOSTNAME}" "${REMOTE_MYSQL_PORT}" "${REMOTE_MYSQL_USER}" "${REMOTE_MYSQL_PASSWORD}"
    Database::setOptions dbRemoteInstance "${MYSQL_OPTIONS} --connect-timeout=5"
fi
# check if local db exists
LOCAL_DB_EXISTS=0
Database::ifDbExists dbLocalInstance "${LOCAL_DB}" && {
    Log::displayInfo "Local Database ${LOCAL_DB} already exists !"
    if [[ "${FORCE}" = "0" ]]; then
        Log::displayError "use --force to drop it"
        exit 1
    fi
    LOCAL_DB_EXISTS='1'
}

if [[ -z "${DB_IMPORT_DUMP_DIR}" ]]; then
    Log::displayError "you have to specify a value for DB_IMPORT_DUMP_DIR env variable"
    exit 1
fi
# remove last slash
DB_IMPORT_DUMP_DIR=${DB_IMPORT_DUMP_DIR%/}

if [[ ! -d "${DB_IMPORT_DUMP_DIR}" ]]; then
    mkdir -p "${DB_IMPORT_DUMP_DIR}" || {
        Log::displayError "impossible to create directory ${DB_IMPORT_DUMP_DIR} specified by DB_IMPORT_DUMP_DIR env variable"
        exit 1
    }
fi

S3_PREFIX=""
if [[ "${FROM_AWS}" = "1" ]]; then
    S3_PREFIX="S3_"
fi
REMOTE_DB_DUMP_TEMP_FILE="${DB_IMPORT_DUMP_DIR}/${S3_PREFIX}${REMOTE_DB}_${PROFILE}.sql"
REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE="${DB_IMPORT_DUMP_DIR}/${S3_PREFIX}${REMOTE_DB}_${PROFILE}_structure.sql"

# check if local dump exists
if [[ "${DOWNLOAD_DUMP}" = "0" ]]; then
    if [[ ! -f "${REMOTE_DB_DUMP_TEMP_FILE}" ]]; then
        Log::displayInfo "local dump does not exist"
        DOWNLOAD_DUMP=1
    fi
    if [[ "${FROM_AWS}" = "0" && ! -f "${REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE}" ]]; then
        Log::displayInfo "local structure dump does not exist"
        DOWNLOAD_DUMP=1
    fi
    if [[ "${DOWNLOAD_DUMP}" = "0" ]]; then
        Log::displayInfo "local dump already exists, avoid download"
    fi
fi

# dump header/footer
read -r -d '\0' DUMP_HEADER <<- EOM
    SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS = 0;
    SET @OLD_AUTOCOMMIT=@@AUTOCOMMIT, AUTOCOMMIT = 0;
    SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS = 0;\0
EOM

read -r -d '\0' DUMP_FOOTER <<- EOM2
    COMMIT;
    SET AUTOCOMMIT=@OLD_AUTOCOMMIT;
    SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;
    SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;\0
EOM2

Log::displayInfo "Importing remote db '${REMOTE_DB}' to local db '${LOCAL_DB}'"
if [[ "${DOWNLOAD_DUMP}" = "1" ]]; then
    Log::displayInfo "Download dump"

    # get remote db collation name
    if [[ -z "${COLLATION_NAME}" ]]; then
        COLLATION_NAME=$(Database::query dbRemoteInstance \
            "SELECT default_collation_name FROM information_schema.SCHEMATA WHERE schema_name = \"${REFERENCE_DB}\";" "information_schema")
    fi

    # get remote db character set
    if [[ -z "${CHARACTER_SET}" ]]; then
        CHARACTER_SET=$(Database::query dbRemoteInstance \
            "SELECT default_character_set_name FROM information_schema.SCHEMATA WHERE schema_name = \"${REFERENCE_DB}\";" "information_schema")
    fi

    DUMP_HEADER=$(printf "${DUMP_HEADER}\nSET names '${CHARACTER_SET}';\n")

    # check if remote db exists
    Database::ifDbExists dbRemoteInstance "${REFERENCE_DB}" || {
        Log::displayError "Remote Database ${REFERENCE_DB} does not exist"
        exit 1
    }

    # calculate remote db dump size
    Log::displayInfo "Calculate tables list for profile ${PROFILE}"
    chmod 755 "${PROFILE_COMMAND}"

    FULL_LIST_TABLES="$(Database::query dbRemoteInstance "show tables" "${REFERENCE_DB}")"
    LIST_TABLES="$(echo "${FULL_LIST_TABLES}" | ${PROFILE_COMMAND})"
    readarray -t LIST_TABLES_ARRAY <<< ${LIST_TABLES}
    IFS=$'\n' eval 'LIST_TABLES_ARRAY=(${LIST_TABLES})'
    
    if [[ "${FROM_AWS}" = "1" ]]; then
        # dowload dump from s3
        S3_URL="${S3_BASE_URL%/}/${REMOTE_DB}.tar.gz"
        (
            set -o errexit
            cd '/tmp' || exit 1 
            Log::displayInfo "Download dump from ${S3_URL} ...";
            aws s3 cp "${S3_URL}" . || exit 1 
            tar xvzf "${REMOTE_DB}.tar.gz" || exit 1 
            rm "${REMOTE_DB}.tar.gz" || exit 1 
            # filter insert matching profile
            SED_OPTIONS=("-i")
            LIST_REMOVED_TABLES="$(diff --changed-group-format="%<" <(echo "${FULL_LIST_TABLES}") <(echo "${LIST_TABLES}"))"
            Log::displayInfo "Keeping inserts for tables : $(printf "'%q'," ${LIST_TABLES})"
            while IFS=$'\n' read -r TABLE; do
                SED_OPTIONS+=("-e")
                SED_OPTIONS+=("s/^INSERT INTO \`${TABLE}\` .*$//g")
            done < <(echo "${LIST_REMOVED_TABLES}")
            sed "${SED_OPTIONS[@]}" "${REMOTE_DB}.sql"
            mv "${REMOTE_DB}.sql" "${REMOTE_DB_DUMP_TEMP_FILE}"
        ) || {
            Log::displayError "unable to download dump from S3 : ${S3_URL}"
            exit 1
        }
    else
        # Calculate dump size
        LIST_TABLES_DUMP=$(printf "'%q'," "${LIST_TABLES_ARRAY[@]}")
        Log::displayInfo "Calculate dump size for tables ${LIST_TABLES_DUMP}"
        DUMP_SIZE_QUERY="SELECT ROUND(SUM(data_length + index_length) / 1024 / 1024, 0) AS size FROM information_schema.TABLES WHERE table_schema=\"${REFERENCE_DB}\""
        DUMP_SIZE_QUERY+=" AND table_name IN(${LIST_TABLES_DUMP} 'dummy') "
        DUMP_SIZE_QUERY+=" GROUP BY table_schema"
        REMOTE_DB_DUMP_SIZE=$(echo "${DUMP_SIZE_QUERY}" | Database::query dbRemoteInstance )
        if [[ -z "${REMOTE_DB_DUMP_SIZE}" ]]; then
            # could occur with the none profile
            REMOTE_DB_DUMP_SIZE="0"
        fi

        # dump db
        Log::displayInfo "Dump the database $REMOTE_DB (Size:${REMOTE_DB_DUMP_SIZE}MB) ...";
        echo "${DUMP_HEADER}" > "${REMOTE_DB_DUMP_TEMP_FILE}"
        DUMP_SIZE_PV_ESTIMATION=$(awk "BEGIN {printf \"%.0f\",${REMOTE_DB_DUMP_SIZE}/1.5}")
        time Database::dump dbRemoteInstance "${REMOTE_DB}" "${LIST_TABLES//[,]/ }" --no-create-info --skip-add-drop-table --single-transaction=TRUE | \
            pv --progress --size ${DUMP_SIZE_PV_ESTIMATION}m >> "${REMOTE_DB_DUMP_TEMP_FILE}"
        echo ${DUMP_FOOTER} >> "${REMOTE_DB_DUMP_TEMP_FILE}"

        Log::displayInfo "Dump structure of the database ${REMOTE_DB} ...";
        echo "${DUMP_HEADER}" > "${REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE}"
        time Database::dump dbRemoteInstance "${REMOTE_DB}" "" --no-data --skip-add-drop-table --single-transaction=TRUE \
            >> "${REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE}"
        echo ${DUMP_FOOTER} >> "${REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE}"
    fi
    Log::displayInfo "Dump done.";
else
    if [[ -z "${COLLATION_NAME}" ]]; then
        COLLATION_NAME="utf8_general_ci"
    fi
fi

# drop local db
if [[ "${LOCAL_DB_EXISTS}" = "1" ]]; then
    Log::displayInfo "drop local database ${LOCAL_DB}";
    Database::query dbLocalInstance "DROP DATABASE IF EXISTS ${LOCAL_DB}"
fi

Log::displayInfo "create local database ${LOCAL_DB}";
Database::query dbLocalInstance "CREATE DATABASE ${LOCAL_DB}/*!40100 COLLATE \"${COLLATION_NAME}\" */"

if [[ "${SKIP_SCHEMA}" = "1" ]]; then
    Log::displayInfo "avoid to create db structure";
else
    Log::displayInfo "create db structure from ${REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE}";
    time ( \
        pv "${REMOTE_DB_STRUCTURE_DUMP_TEMP_FILE}" | \
            Database::query dbLocalInstance "" ${LOCAL_DB}
    )
fi

Log::displayInfo "import remote to local from file ${REMOTE_DB_DUMP_TEMP_FILE}"
time ( \
    pv "${REMOTE_DB_DUMP_TEMP_FILE}" | \
        Database::query dbLocalInstance "" ${LOCAL_DB}
)

Log::displayInfo "Done";
